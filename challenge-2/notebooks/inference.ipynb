{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2882d2be",
   "metadata": {},
   "source": [
    "# One-Class Soil Classification: Inference Phase\n",
    "\n",
    "**Author:** Siddhant Bhardwaj  \n",
    "**Team Name:** Siddhant Bhardwaj \n",
    "**Team Members:** Siddhant Bhardwaj, Sivadhanushya\n",
    "**Leaderboard Rank:** 36 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63148930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Author: Siddhant Bhardwaj\n",
    "Team Name: Siddhant Bhardwaj\n",
    "Team Members: Siddhant, Sivadhanushya\n",
    "Leaderboard Rank: 36\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup and Configuration ---\n",
    "print(\"--- [1] Initializing: Importing Libraries and Setting Configuration ---\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt # For displaying sample test images (optional)\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configuration (MUST MATCH TRAINING NOTEBOOK'S CONFIG FOR MODEL AND PERCENTILE)\n",
    "BASE_PATH = '/kaggle/input/soil-classification-part-2/soil_competition-2025'\n",
    "TEST_IDS_PATH = f'{BASE_PATH}/test_ids.csv'\n",
    "TEST_IMG_PATH = f'{BASE_PATH}/test'\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch16\" # Must be same as used in training\n",
    "SIMILARITY_THRESHOLD_PERCENTILE = 5.5         # Must be same as used in training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME_FOR_FILE = CLIP_MODEL_NAME.split('/')[-1].replace(\"-\", \"_\")\n",
    "\n",
    "# Input files from \"training\" phase (ensure these are in /kaggle/working/ or accessible)\n",
    "PROTOTYPE_FILE = f'/kaggle/working/soil_prototype_{MODEL_NAME_FOR_FILE}.npy'\n",
    "THRESHOLD_FILE = f'/kaggle/working/similarity_threshold_{MODEL_NAME_FOR_FILE}_p{SIMILARITY_THRESHOLD_PERCENTILE:.1f}.txt'\n",
    "\n",
    "# Output file\n",
    "SUBMISSION_FILENAME = f'/kaggle/working/submission_{MODEL_NAME_FOR_FILE}_inference_p{SIMILARITY_THRESHOLD_PERCENTILE:.1f}.csv'\n",
    "NUM_SAMPLE_TEST_IMAGES_TO_DISPLAY = 3\n",
    "\n",
    "\n",
    "print(f\"Device: {DEVICE}, CLIP Model: {CLIP_MODEL_NAME}\")\n",
    "print(f\"Using prototype: {PROTOTYPE_FILE}\")\n",
    "print(f\"Using threshold file: {THRESHOLD_FILE}\")\n",
    "\n",
    "# In a Jupyter Notebook Cell 3 (Code)\n",
    "# --- 2. Load CLIP Model & Processor ---\n",
    "print(\"\\n--- [2] Loading CLIP Model ---\")\n",
    "try:\n",
    "    clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(DEVICE)\n",
    "    clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
    "    clip_model.eval()\n",
    "    print(\"CLIP model and processor loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CLIP model: {e}\"); raise e\n",
    "\n",
    "# In a Jupyter Notebook Cell 4 (Code)\n",
    "# --- 3. Load Pre-calculated Prototype and Threshold ---\n",
    "print(\"\\n--- [3] Loading Prototype and Threshold ---\")\n",
    "try:\n",
    "    soil_prototype = np.load(PROTOTYPE_FILE)\n",
    "    with open(THRESHOLD_FILE, 'r') as f:\n",
    "        similarity_threshold = float(f.read())\n",
    "    print(f\"Soil prototype loaded. Shape: {soil_prototype.shape}\")\n",
    "    print(f\"Similarity threshold loaded: {similarity_threshold:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading prototype or threshold: {e}\"); raise e\n",
    "\n",
    "# In a Jupyter Notebook Cell 5 (Code)\n",
    "# --- 4. Load Test Data & Display Samples ---\n",
    "print(\"\\n--- [4] Loading Test Data ---\")\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_IDS_PATH)\n",
    "    test_image_ids_original = test_df['image_id'].tolist()\n",
    "    if not test_image_ids_original: raise ValueError(\"No test IDs.\")\n",
    "    print(f\"Loaded {len(test_image_ids_original)} test image IDs.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data: {e}\"); raise e\n",
    "\n",
    "# (Code to display sample test images - optional, similar to training)\n",
    "print(\"Displaying sample test images (refer to main script's plotting for full code)...\")\n",
    "\n",
    "\n",
    "# In a Jupyter Notebook Cell 6 (Code)\n",
    "# --- 5. CLIP Embedding Function and Extraction for Test Data ---\n",
    "print(\"\\n--- [5] Extracting Test Image Embeddings ---\")\n",
    "# (Function definition for get_clip_image_embeddings - same as in the training notebook)\n",
    "def get_clip_image_embeddings(image_ids_list, img_directory, model, processor, device, model_name_desc):\n",
    "    embeddings_list = []\n",
    "    valid_ids_processed = []\n",
    "    model.eval()\n",
    "    for img_id in tqdm(image_ids_list, desc=f\"Extracting Embeddings ({model_name_desc})\"):\n",
    "        img_path = os.path.join(img_directory, img_id)\n",
    "        if not os.path.exists(img_path): continue\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(text=None, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(pixel_values=inputs.pixel_values)\n",
    "            embeddings_list.append(image_features.cpu().numpy().flatten())\n",
    "            valid_ids_processed.append(img_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing '{img_id}': {e}. Skipping.\")\n",
    "            continue\n",
    "    if not embeddings_list: return np.array([]), []\n",
    "    return np.array(embeddings_list), valid_ids_processed\n",
    "\n",
    "test_clip_embeddings, test_image_ids_processed_for_submission = np.array([]), []\n",
    "if test_image_ids_original:\n",
    "    test_clip_embeddings, test_image_ids_processed_for_submission = get_clip_image_embeddings(\n",
    "        test_image_ids_original, TEST_IMG_PATH, clip_model, clip_processor, DEVICE, MODEL_NAME_FOR_FILE\n",
    "    )\n",
    "print(f\"Test embeddings shape: {test_clip_embeddings.shape}\")\n",
    "\n",
    "# In a Jupyter Notebook Cell 7 (Code)\n",
    "# --- 6. Classification and Submission File Generation ---\n",
    "print(\"\\n--- [6] Classification and Submission ---\")\n",
    "test_labels = []\n",
    "if test_clip_embeddings.shape[0] > 0 and len(test_image_ids_processed_for_submission) > 0:\n",
    "    print(\"Classifying test images...\")\n",
    "    similarities_to_prototype_test = cosine_similarity(\n",
    "        test_clip_embeddings, soil_prototype.reshape(1, -1)\n",
    "    ).flatten()\n",
    "    test_labels = [1 if sim >= similarity_threshold else 0 for sim in similarities_to_prototype_test]\n",
    "else:\n",
    "    print(\"No test embeddings to classify.\")\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'image_id': test_image_ids_processed_for_submission, \n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "if not submission_df.empty:\n",
    "    print(\"\\nPredicted label distribution in submission:\")\n",
    "    print(submission_df['label'].value_counts(normalize=True).to_string())\n",
    "else:\n",
    "    print(\"\\nSubmission DataFrame is empty.\")\n",
    "\n",
    "submission_df.to_csv(SUBMISSION_FILENAME, index=False)\n",
    "print(f\"\\nSubmission file '{SUBMISSION_FILENAME}' created in /kaggle/working/.\")\n",
    "print(\"--- Inference Phase Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
