{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training notebook: EfficientNet-B1 + Optuna tuning + final training ---\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_INPUT_PATH = '/kaggle/input/soil-classification/soil_classification-2025'\n",
    "TRAIN_DIR = os.path.join(BASE_INPUT_PATH, 'train')\n",
    "TEST_DIR = os.path.join(BASE_INPUT_PATH, 'test')\n",
    "TRAIN_LABELS_CSV = os.path.join(BASE_INPUT_PATH, 'train_labels.csv')\n",
    "TEST_IDS_CSV = os.path.join(BASE_INPUT_PATH, 'test_ids.csv')\n",
    "OUTPUT_DIR = '/kaggle/working/'\n",
    "\n",
    "IMG_SIZE = 240\n",
    "NUM_CLASSES = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N_OPTUNA_TRIALS = 25\n",
    "EPOCHS_PER_TRIAL = 12\n",
    "FINAL_MODEL_EPOCHS = 30\n",
    "STUDY_SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(STUDY_SEED)\n",
    "\n",
    "# 1. Load Data & EDA\n",
    "df_train = pd.read_csv(TRAIN_LABELS_CSV)\n",
    "df_test_ids = pd.read_csv(TEST_IDS_CSV)\n",
    "class_names = sorted(df_train['soil_type'].unique())\n",
    "label_to_int = {lbl:i for i,lbl in enumerate(class_names)}\n",
    "int_to_label = {i:lbl for lbl,i in label_to_int.items()}\n",
    "df_train['label_int'] = df_train['soil_type'].map(label_to_int)\n",
    "print(\"Class distribution:\\n\", df_train['soil_type'].value_counts())\n",
    "\n",
    "# 2. Preprocessing & Transforms\n",
    "mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(), T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(45),\n",
    "    T.ColorJitter(0.3,0.3,0.3,0.15),\n",
    "    T.RandomAffine(0, translate=(0.1,0.1), scale=(0.9,1.1), shear=10),\n",
    "    T.ToTensor(), T.Normalize(mean,std)\n",
    "])\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    T.ToTensor(), T.Normalize(mean,std)\n",
    "])\n",
    "\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "        if not is_test:\n",
    "            self.labels = df['label_int'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.dir, row['image_id'])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            img = Image.new('RGB',(IMG_SIZE,IMG_SIZE),color='black')\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        if self.is_test:\n",
    "            return img, row['image_id']\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# 3. Split train/val\n",
    "train_df, val_df = train_test_split(\n",
    "    df_train, test_size=0.2,\n",
    "    stratify=df_train['label_int'],\n",
    "    random_state=STUDY_SEED\n",
    ")\n",
    "train_ds = SoilDataset(train_df, TRAIN_DIR, train_transforms)\n",
    "val_ds   = SoilDataset(val_df,   TRAIN_DIR, val_transforms)\n",
    "\n",
    "# 4. Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\",1e-5,1e-3,log=True)\n",
    "    opt_name = trial.suggest_categorical(\"optimizer\",[\"AdamW\",\"Adam\"])\n",
    "    bs = trial.suggest_categorical(\"batch_size\",[16,32])\n",
    "    wd = trial.suggest_float(\"weight_decay\",1e-6,1e-2,log=True)\n",
    "    unfreeze = trial.suggest_int(\"num_unfreeze_blocks\",1,4)\n",
    "    sched_pat = trial.suggest_int(\"sched_patience\",2,4)\n",
    "    sched_fac = trial.suggest_float(\"sched_factor\",0.1,0.5)\n",
    "\n",
    "    # DataLoaders\n",
    "    try:\n",
    "        train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,\n",
    "                                  num_workers=2, pin_memory=True, drop_last=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=bs, shuffle=False,\n",
    "                                  num_workers=2, pin_memory=True)\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.TrialPruned()\n",
    "        raise\n",
    "\n",
    "    # Model setup\n",
    "    model = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.IMAGENET1K_V1)\n",
    "    for p in model.parameters(): p.requires_grad=False\n",
    "    in_f = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(in_f, NUM_CLASSES)\n",
    "    for p in model.classifier.parameters(): p.requires_grad=True\n",
    "    total = len(model.features)\n",
    "    for i in range(total-unfreeze, total):\n",
    "        for p in model.features[i].parameters():\n",
    "            p.requires_grad=True\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = (optim.AdamW if opt_name==\"AdamW\" else optim.Adam)(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr, weight_decay=wd\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=sched_fac, patience=sched_pat, verbose=False\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_min_f1 = 0.0\n",
    "    for epoch in range(EPOCHS_PER_TRIAL):\n",
    "        model.train()\n",
    "        for X,y in train_loader:\n",
    "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        preds, labs = [], []\n",
    "        with torch.no_grad():\n",
    "            for X,y in val_loader:\n",
    "                X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "                _, p = model(X).max(1)\n",
    "                preds += p.cpu().tolist()\n",
    "                labs += y.cpu().tolist()\n",
    "\n",
    "        f1s = f1_score(labs, preds, average=None,\n",
    "                       labels=list(range(NUM_CLASSES)), zero_division=0)\n",
    "        cur_min = float(np.min(f1s))\n",
    "        best_min_f1 = max(best_min_f1, cur_min)\n",
    "        scheduler.step(cur_min)\n",
    "\n",
    "        trial.report(cur_min, epoch)\n",
    "        if trial.should_prune():\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return best_min_f1\n",
    "\n",
    "# 5. Run study & final retrain\n",
    "pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource=1, reduction_factor=4)\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner, study_name=\"soil_effnet_b1\")\n",
    "study.optimize(objective, n_trials=N_OPTUNA_TRIALS, timeout=10800)\n",
    "best = study.best_trial.params\n",
    "\n",
    "# Final model retraining\n",
    "train_loader = DataLoader(train_ds, batch_size=best['batch_size'], shuffle=True,\n",
    "                          num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=best['batch_size'], shuffle=False,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "model_fin = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.IMAGENET1K_V1)\n",
    "for p in model_fin.parameters(): p.requires_grad=False\n",
    "in_f = model_fin.classifier[1].in_features\n",
    "model_fin.classifier[1] = nn.Linear(in_f, NUM_CLASSES)\n",
    "for p in model_fin.classifier.parameters(): p.requires_grad=True\n",
    "for i in range(len(model_fin.features)-best['num_unfreeze_blocks'], len(model_fin.features)):\n",
    "    for p in model_fin.features[i].parameters():\n",
    "        p.requires_grad=True\n",
    "model_fin.to(DEVICE)\n",
    "\n",
    "optimizer_fin = (optim.AdamW if best['optimizer']==\"AdamW\" else optim.Adam)(\n",
    "    [p for p in model_fin.parameters() if p.requires_grad],\n",
    "    lr=best['lr'], weight_decay=best['weight_decay']\n",
    ")\n",
    "scheduler_fin = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_fin, mode='max', factor=best['sched_factor'],\n",
    "    patience=best['sched_patience'], verbose=True\n",
    ")\n",
    "criterion_fin = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "for epoch in range(FINAL_MODEL_EPOCHS):\n",
    "    model_fin.train()\n",
    "    for X,y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{FINAL_MODEL_EPOCHS}\"):\n",
    "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer_fin.zero_grad()\n",
    "        loss = criterion_fin(model_fin(X), y)\n",
    "        loss.backward()\n",
    "        optimizer_fin.step()\n",
    "\n",
    "    model_fin.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for X,y in val_loader:\n",
    "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "            _, p = model_fin(X).max(1)\n",
    "            preds += p.cpu().tolist()\n",
    "            labs += y.cpu().tolist()\n",
    "\n",
    "    f1s = f1_score(labs, preds, average=None,\n",
    "                   labels=list(range(NUM_CLASSES)), zero_division=0)\n",
    "    cur_min = float(np.min(f1s))\n",
    "    print(f\"Epoch {epoch+1} min F1: {cur_min:.4f}\", f1s)\n",
    "    scheduler_fin.step(cur_min)\n",
    "\n",
    "    if cur_min > best_val_f1:\n",
    "        best_val_f1 = cur_min\n",
    "        torch.save(model_fin.state_dict(), os.path.join(OUTPUT_DIR, \"best_effnet_b1.pth\"))\n",
    "\n",
    "print(\"Training complete, best min-class F1:\", best_val_f1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
